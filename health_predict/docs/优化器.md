# Adam

\# 从 keras.optimizers 导入 Adam 模块

from keras.optimizers import Adam

## 算法

![1531451640335](D:\Manshy\DFC_ML\health_predict\docs\adam-algorithm.png)



class Adam(Optimizer):

​    """

​    # 参数

​        lr: float >= 0. 学习速率、学习步长，值越大则表示权值调整动作越大，对应上图算法中的参数 alpha；

​        beta_1:  接近 1 的常数，（有偏）一阶矩估计的指数衰减因子；

​        beta_2:  接近 1 的常数，（有偏）二阶矩估计的指数衰减因子；

​        epsilon: 大于但接近 0 的数，放在分母，避免除以 0 ；

​        decay:  学习速率衰减因子，【2】算法中无这个参数；

​    """



# SGD

### Time-Based Learning Rate Schedule

epochs = 50

learning_rate = 0.1

decay_rate = learning_rate / epochs

momentum = 0.8

sgd = SGD(lr=learning_rate, momentum=momentum, decay=decay_rate, nesterov=False)

