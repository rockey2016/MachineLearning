

# 架构图

## 时序逻辑结构

下面条件：

**单个层**

**不考虑batch_size(batch_size=1)**

**神经元个数为128**

**输入X向量长度为6**

![1534209811396](D:\Manshy\health_predict\docs\assets\1534209811396.png)

*注：*

1.此图为逻辑结构图，实际物理架构并非如此

2.并不是同一时刻的的实际情况，是在时序中前后参数传入传出的示意图。

## 实际逻辑结构

![1534210005268](D:\Manshy\health_predict\docs\assets\1534210005268.png)

注：

1.此图为一个LSTM cell的内部结构/运算图

2.cell将t-1时刻的输出h(t-1)，状态c(t-1)和输入x(t)三个向量进行运算处理，得到当前时刻的状态和输出

3.X(t)为长度为6的向量(对于batch_size=1)

4.两线相交有关系是实心点，无关是空心点

## 物理架构

![1534210513437](D:\Manshy\health_predict\docs\assets\1534210513437.png)

注：

1.在LSTM Cell中，一个cell 包含了若干个门处理函数，如果每个门看做是由num_hidden个神经元来实现该门函数功能， 那么每个门各自都包含了相应的w参数矩阵以及bias偏置矩阵参数，就是上图物理架构图中的实现。

## 多层神经网络

在RNN中，M个神经元组成的隐含层，实际的功能应该是f(wx + b),  这里实现了两部，首先M个隐含层神经元与输入向量X之间全连接，通过w参数矩阵对x向量进行加权求和，其实就是对x向量各个维度上进行筛选，加上bias偏置矩阵后， *通过f激励函数*， *得到隐含层的输出*。 

![1534211658291](D:\Manshy\health_predict\docs\assets\1534211658291.png)

 **激活函数**

- 必须处处可导（一般都使用S型函数）

**使用S型激活函数时，网络输入与输出关系**

输入： 

![img](D:\Manshy\health_predict\docs\assets\311496-cd69120fcd11d837.png) 

输出：

![img](D:\Manshy\health_predict\docs\assets\311496-788304add8a4d42b.png) 

输出的导数 ：

![img](D:\Manshy\health_predict\docs\assets\311496-9f9d80f30847ff5f.png) 

使用S型激活函数时，网络的输出及其导数图形： 

![1534211933542](D:\Manshy\health_predict\docs\assets\1534211933542.png)

根据S激活函数的图形：

- net在 -5~0 的时候导数的值为正，且导数的值逐渐增大，**说明此时f(x)在逐渐变大 且 变大的速度越来越快** 
- net在 0~5  的时候导数的值为正，且导数的值逐渐减小，**说明此时f(x)在逐渐变大 但是 变大的速度越来越慢** 

对神经网络进行训练，我们应该尽量将net的值尽量控制在收敛比较快的范围内

# 标准学习算法

 **网络结构**

 输入层有`n`个神经元，隐含层有`p`个神经元，输出层有`q`个神经元 

 **变量定义** 

 ![img](D:\Manshy\health_predict\docs\assets\311496-d2970e6fdc8b59d2.png) 

![img](D:\Manshy\health_predict\docs\assets\311496-e2ab9e675c4536e9.png) 

##  学习过程

 **第一步，网络初始化**

给各连接权值分别赋一个区间`（-1,1）`内的随机数，设定误差函数`e`，给定计算精度值`ε`和最大学习次数`M`

 **第二步，随机选取第k个输入样本以及对应的期望输出**

$x(k)=(x~1~(k),x~2~(k),...,x~n~(k))$

$d~o~(k)=(d~1~(k),d~2~(k),...,d~q~(k))$

 **第三步，计算隐含层各神经元的输入和输出**

![img](D:\Manshy\health_predict\docs\assets\311496-10e78c58a19dc937.png) 

**第四步，利用网络期望输出和实际输出，计算误差函数对输出层的各神经元的偏导数**![img](D:\Manshy\health_predict\docs\assets\311496-8d9d69c91f02f61f.png) 

可以列出如下等式 :

1.误差函数对w的导数为误差函数先对输出层输入求导，再用输出层输入对w求导。（输入是w的函数）

![img](D:\Manshy\health_predict\docs\assets\311496-c7e3f502fc2cf4d8.png) 

2.输入对w求导

![img](D:\Manshy\health_predict\docs\assets\311496-a89ae136ad18c166.png) 

3.误差函数对输出层输入求导

![img](D:\Manshy\health_predict\docs\assets\311496-520c740ace64f0c0.png) 

***最下面那个方框是’=‘号*** 

**第五步，利用隐含层到输出层的连接权值、输出层的![img](D:\Manshy\health_predict\docs\assets\311496-8d9d69c91f02f61f-1534213633372.png)和隐含层的输出计算误差函数对隐含层各神经元的偏导数**![img](D:\Manshy\health_predict\docs\assets\311496-3098a88a2f643735.png) 。

1.![img](D:\Manshy\health_predict\docs\assets\311496-36e215bf506f1622.png) 

2.误差函数对输入层到隐藏层的权重求偏导数

![img](D:\Manshy\health_predict\docs\assets\311496-9e73fbcc520f5aaa.png) 

3.过程与第四步类似

![img](D:\Manshy\health_predict\docs\assets\311496-a5971b30f0f2c1c5.png) 

![img](D:\Manshy\health_predict\docs\assets\311496-cab4939f56cc7e87.png) 

***最下面那个方框是’=‘号*** 

**第六步，利用输出层各神经元的![img](D:\Manshy\health_predict\docs\assets\311496-d11605010a47e0bd.png)和隐含层各神经元的输出来修正连接权值![img](D:\Manshy\health_predict\docs\assets\311496-1b4e5096764ec8c1.png)**

1.梯度

![img](D:\Manshy\health_predict\docs\assets\311496-23e016f9bc21237e.png) 

2.迭代并更新权重

![img](D:\Manshy\health_predict\docs\assets\311496-1275bf9aec0cdfa9.png) 

**第七步，利用隐含层各神经元的![img](D:\Manshy\health_predict\docs\assets\311496-312a9c907d4661d9.png)和输入层各神经元的输入参数修正连接权值**。

![img](D:\Manshy\health_predict\docs\assets\311496-d8f241f19e8742aa.png) 

![img](D:\Manshy\health_predict\docs\assets\311496-5d3a02cdb90f0490.png) 

 **第八步，计算全局误差**

![img](D:\Manshy\health_predict\docs\assets\311496-41528f58e602b184.png) 

**第九步，判断网络误差是否满足要求，判断网络误差是否满足要求。当误差达到预设精度或学习次数大于设定的最大次数，则结束算法。否则，选取下一个学习样本及对应的期望输出，返回到第三步，进入下一轮学习**。

## 算法直观解释

当误差对权值的偏导数大于零时，权值调整量为负，实际输出大于期望输出，权值向减少方向调整，使得实际输出与期望输出的差减少。

![img](D:\Manshy\health_predict\docs\assets\311496-811ffa1d694608cf.png) 

当误差对权值的偏导数小于零时，权值调整量为正，实际输出少于期望输出，权值向增大方向调整，使得实际输出与期望输出的差减少。 

![img](D:\Manshy\health_predict\docs\assets\311496-e20bb9783263595d.png) 



# 神经网络

神经网络的本质就是通过参数与激活函数来拟合特征与目标之间的真实函数关系。 

## 激活层

激活层存在的最大目的就是引入非线性因素，以增加整个网络的表征能力。

选取合适的“激活函数”就显得非常重要了。

常用的激活函数Sigmoid（或tanh函数），（如图12-3所示）。 

![1534228202842](D:\Manshy\health_predict\docs\assets\1534228202842.png)



Sigmoid之类激活函数有个很大的缺点，就是它的导数值很小。比如说，Sigmoid的导数取值范围仅为[0, 1/4]。且当输入数据（*x*）很大或者很小的时候，其导数趋都近于0。这就意味着，很容易产生所谓的梯度消失（vanishing gradient）现象。没有了梯度的指导，那么神经网络的参数训练，就如同“无头的苍蝇”，毫无方向感。 



如何防止深度神经网络陷入梯度消失，或说如何提升网络的训练效率，一直都是深度学习非常热门的研究课题。

在卷积神经网络中，最常用的激活函数久是修正线性单元(Rectified Linear Unit，简称ReLU) 

![1534228417293](D:\Manshy\health_predict\docs\assets\1534228417293.png)



**激活函数：**

使用：

from keras.layers import Activation, Dense

model.add(Dense(64))
model.add(Activation('tanh'))

等价于：

`model.add(Dense(64, activation='tanh'))`

也可以

`from keras import backend as K`

`model.add(Dense(64, activation=K.tanh))`

**分类：**

‘relu’

‘hard_sigmoid ’



## BatchNormalization 层:

通常在线性向非线性转变时使用,如下

```
model.add(Dense(100,input_dim=20))
model.add(BatchNormalization())
model.add(Activation('relu'))
```

作用： 能够保证权重的尺度不变，因为BatchNormalization在激活函数前对输入进行了标准化 



## RNN梯度爆炸和梯度消失

为什么RNN会产生梯度爆炸和消失问题呢 ?

![1534229264320](D:\Manshy\health_predict\docs\assets\1534229264320.png)

上式的定义为矩阵的模的上界。因为上式是一个指数函数，如果t-k很大的话（也就是向前看很远的时候），会导致对应的**误差项**的值增长或缩小的非常快，这样就会导致相应的**梯度爆炸**和**梯度消失**问题（取决于大于1还是小于1）。

通常来说，**梯度爆炸**更容易处理一些。因为梯度爆炸的时候，我们的程序会收到NaN错误。我们也可以设置一个梯度阈值，当梯度超过这个阈值的时候可以直接截取。

**梯度消失**更难检测，而且也更难处理一些。总的来说，我们有三种方法应对梯度消失问题：

1. 合理的初始化权重值。初始化权重，使每个神经元尽可能不要取极大或极小值，以躲开梯度消失的区域。
2. 使用relu代替sigmoid和tanh作为激活函数。
3. 使用其他结构的RNNs，比如长短时记忆网络（LTSM）和Gated Recurrent Unit（GRU），这是最流行的做法。我们将在以后的文章中介绍这两种网络。

## 全连接层

传统机器学习方法在建模的时候，一般以赋权重的方式表示每一个特征对最终目标造成的影响，全连接层也是这种作用，它是**一种组合底层网络学习到的特征到目标的方式**。其输入是一系列高度抽象后的feature map，全连接层就是对这些特征进行投票。 





# 参考：

https://blog.csdn.net/shenxiaoming77/article/details/79390595

[简述]https://www.jianshu.com/p/4d37813c0952

[梯度]https://zybuluo.com/hanbingtao/note/541458

[keras]https://cloud.tencent.com/developer/article/1010815

[BN]https://www.cnblogs.com/guoyaohua/p/8724433.html