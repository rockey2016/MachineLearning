

# AdaBoost算法

## 基本思想

- 每一轮如何改变训练数据的权值或者概率分布

提高那些被前一轮弱分类器错误分类的样本的权值，而降低那些被正确分类样本的权值 。

- 如何将若分类器组合成一个强分类器

采取加权多数表决的方法，加大分类误差率较小的弱分类器的权值，使其在表决中起较大的作用，减小分类误差率大的弱分类器的权值，使其在表决中起较小的作用。

## 算法流程

假定给定一个二分类的训练数据集

$T={(x1,y1),(x2,y2),⋯,(xN,yN)}$

其中，每个样本点由实例与标记组成. 实例 $xi∈X⊆Rn$ ，标记 $yi∈Y⊆Rn$， X 是实例空间，Y 是标记集合. 

输入：训练数据集 $T={(x1,y1),(x2,y2),⋯,(xN,yN)}$，其中 $xi∈X⊆Rn， yi∈Y={−1,+1}$；弱分类器；

输出：最终分类器 $G(x)$

## 模型演示

![1533866320590](D:\Manshy\OpenProject\MachineLearning\docs\assets\1533866320590.png)

上图是一个Boosting的过程，绿色的线表示目前取得的模型（模型是由前m次得到的模型合并得到的），虚线表示当前这次模型。每次分类的时候，会更关注分错的数据，上图中，红色和蓝色的点就是数据，点越大表示权重越高，看看右下角的图片，当m=150的时候，获取的模型已经几乎能够将红色和蓝色的点区分开了。 

## 总结

![1533866603442](D:\Manshy\OpenProject\MachineLearning\docs\assets\1533866603442.png)

训练集中一共有n个点，在每一个点赋上一个权重Wi(0 <= i < n)，表示这个点的重要程度。初始时每个点的权重都是一样的

*上图中绿线表示依次训练模型*，通过依次训练模型的过程，对点的权重进行修正，如果分类正确了，权重降低，如果分类错了，则权重提高。可以想象得到，程序越往后执行，训练出的模型就越会在意那些容易分错（权重高）的点。

当全部的程序执行完后，会得到M个模型，分别对应上图的y1(x)…yM(x)，通过加权的方式组合成一个最终的模型YM(x)。 



# 决策树

## 概念

**ID3算法** ：以信息增益作为划分训练数据集的特征的算法称为

**C4.5算法**：用信息增益比来选择特征的算法

**信息增益（information gain）** ：

特征A对训练数据集D的信息增益$g(D,A)$ 

​	g(D,A)=H(D)−H(D|A)

这个差又称为互信息。信息增益大的特征具有更强的分类能力。 

 根据信息增益准则的特征选择方法是：对训练数据集（或子集）计算其每个特征的信息增益，选择信息增益最大的特征。 

 计算信息增益的算法如下：  

输入：训练数据集DD和特征A；  

输出：特征A对训练数据集D的信息增益$g(D,A)$.  

**信息增益比（information gain ratio）**  

特征A对训练数据集D的信息增益比$gR(D,A)$定义为其信息增益$g(D,A)$与训练数据集D关于特征A的值的熵$HA(D)$之比，即

$gR(D,A)=g(D,A)/HA(D)$

其中，

$HA(D)=−∑ni=1(|Di|/|D|)log2(|Di|/|D|)$，

n是特征A取值的个数。

## 基本思想

*<u>**如何按次序选择属性**</u>* 

树根上以及树节点是哪个变量呢？怎么衡量这些变量的重要性呢？

**ID3算法**用的是信息增益，**C4.5算法**用信息增益率；**CART算法**使用基尼系数。决策树方法是会把每个特征都试一遍，然后选取能够使分类分得最好的那个特征，例如将A属性作为父节点，产生的纯度增益（GainA）要大于B属性作为父节点，则A作为优先选取的属性。 

![1533806327318](D:\Manshy\OpenProject\MachineLearning\docs\assets\1533806327318.png)

 <u>**如何分裂训练数据（对每个属性选择最优的分割点）**</u> 

**CART**算法：既可以做分类，也可以做回归。只能形成二叉树。 

**损失函数**：其实这里的损失函数，就是分类的准则，也就是求最优化的准则。

对于分类树（目标变量为离散变量）：同一层所有分支假设函数的基尼系数的平均。

对于回归树（目标变量为连续变量）：同一层所有分支假设函数的平方差损失

数据集D的基尼系数Gini(D)反映了从数据集D中随机抽取两个样本，其类别标记不一致的概率。因此Gini(D)越小，则数据集D的纯度越高。

**三种方法对比：**

**ID3的缺点，倾向于选择水平数量较多的变量，可能导致训练得到一个庞大且深度浅的树；另外输入变量必须是分类变量（连续变量必须离散化）；最后无法处理空值。**

**C4.5选择了信息增益率替代信息增益。**

**CART以基尼系数替代熵；最小化不纯度而不是最大化信息增益**。 

<u>**如何停止分裂**</u> 

 预剪枝，及早的停止树增长控制树的规模，方法可以参考如下6点停止分类的条件。后剪枝在已生成过拟合决策树上进行剪枝，删除没有意义的组，可以得到简化版的剪枝决策树，包括REP（设定一定的误分类率，减掉对误分类率上升不超过阈值的多余树）、PEP，还有一种CCP，即给分裂准则—基尼系数加上惩罚项，此时树的层数越深，基尼系数的惩罚项会越大。 

![1533807613542](D:\Manshy\OpenProject\MachineLearning\docs\assets\1533807613542.png)







# **Gradient Boost Decision Tree**

**Gradient** Boost与传统的Boost的区别 ：

每一次的计算是为了减少上一次的残差(residual)，而为了消除残差，我们可以在**残差减少的梯度(Gradient)方向**上建立一个新的模型。

在Gradient Boost中，每个新的模型的建立是为了使得之前模型的残差往梯度方向减少；与传统Boost对正确、错误的样本进行加权有着很大的区别。 

## 算法

![1533869368186](D:\Manshy\OpenProject\MachineLearning\docs\assets\1533869368186.png)

0. 表示给定一个初始值

​     1. 表示建立M棵决策树（迭代M次）

​     2. 表示对函数估计值F(x)进行Logistic变换

​     3. 表示对于K个分类进行下面的操作（其实这个for循环也可以理解为向量的操作，每一个样本点xi都对应了K种可能的分类yi，所以yi, F(xi), p(xi)都是一个K维的向量，这样或许容易理解一点）

​     4. 表示求得残差减少的梯度方向

​     5. 表示根据每一个样本点x，与其残差减少的梯度方向，得到一棵由J个叶子节点组成的决策树

6. 为当决策建立完成后，通过这个公式，可以得到每一个叶子节点的增益（这个增益在预测的时候用的）

7. 将当前得到的决策树与之前的那些决策树合并起来，作为新的一个模型(跟6中所举的例子差不多) 



# 随机森林

## 基本思想

随机森林是用随机的方式建立一个森林，森林里面有很多的决策树组成，每一棵决策树之间没有关联。当有一个新输入样本进入时，就让森林中的每一棵决策树分别进行一下判断，看看这个样本应该属于哪一类（对于分类算法），然后看看哪一类被选择最多，就预测这个样本为那一类。

1.2 随机森林优点

同一批数据，用同样的算法只能产生一棵树，这时Bagging策略可以帮助我们产生不同的数据集。Bagging策略来源于bootstrap aggregation：从样本集（假设样本集N个数据点）中重采样选出Nb个样本（有放回的采样，样本数据点个数仍然不变为N），在所有样本上，对这n个样本建立分类器（ID3\C4.5\CART\SVM\LOGISTIC），重复以上两步m次，获得m个分类器，最后根据这m个分类器的投票结果，决定数据属于哪一类。 

**随机森林在bagging**的基础上更进一步：

**1.**  **样本的随机：从样本集中用Bootstrap**随机选取n个样本

**2.**  **特征的随机**：从所有属性中随机选取K个属性，选择最佳分割属性作为节点建立CART决策树（泛化的理解，这里面也可以是其他类型的分类器，比如SVM、Logistics）

**3.**  重复以上两步m次，即建立了m棵CART决策树

**4.**  这m个CART形成随机森林，通过投票表决结果，决定数据属于哪一类（投票机制有一票否决制、少数服从多数、加权多数）

**关于调参**：

​		1.如何选取K，可以考虑有N个属性，取K=根号N

​               **2.**最大深度（不超过8层）

​               **3.**棵数

​               **4.**最小分裂样本树

​               **5.**类别比例

# 参考：

https://www.cnblogs.com/fionacai/p/5894142.html

https://blog.csdn.net/HerosOfEarth/article/details/52347820

https://www.cnblogs.com/90zeng/p/adaboost.html#contents

https://blog.csdn.net/u010454729/article/details/49120795

https://ljalphabeta.gitbooks.io/python-/content/randomforest.html

集成学习方法：boosting和bagging

https://blog.csdn.net/google19890102/article/details/51746402/

https://blog.csdn.net/program_developer/article/details/79404581